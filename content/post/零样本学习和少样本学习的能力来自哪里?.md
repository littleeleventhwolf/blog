---
title: "零样本学习和少样本学习的能力来自哪里?"
date: 2026-02-27T09:43:31+08:00
lastmod: 2026-02-27T09:43:31+08:00
keywords: ["llm"]
categories: ["llm"]
tags: ["llm"]
author: "小十一狼"
---

# 训练阶段

直观上看，大模型的目标是“预测下一个词”，但在海量、高质量的数据上进行这种预测，实际上能“逼迫”模型学会：

- **语言规律**：语法、搭配
- **世界知识**：事实性信息（比如“北京是中国的首都”）
- **任务逻辑**：数据中天然包含的“问题-答案”、“原文-摘要”、“英语-中文”的配对

关键点在于为了把下一个词预测得足够准（Loss 足够低），模型必须在它的参数中内化这些逻辑。

# 大规模参数带来的能力涌现

- **小模型**：能做简单的“完形填空”。比如看到“这只猫很”，后面可以接一个“温顺”。但是不具备推理能力。
- **大模型**：当参数量突破某个临界点，模型好像开始“顿悟”了。

由于大模型有个足够的参数来存储复杂的模式和逻辑链，就好像人开发了更大的脑容量一样，就好像电影《超体》。

注意力机制中的 Q/K/V，直觉去比拟的话，就像 Question 在巨大的知识库里找到相关的 Key，并提取出对应的 Value。当这个知识库大到包含了人类的大部分知识时，它就能通过组合这些知识来完成新任务。

# 上下文学习

提示词（Prompt）就是激发大模型隐藏状态的“咒语”。

1. **零样本学习**：

直接问：“把「你好」翻译成韩语。”

模型在训练时见过无数“指令-执行”的文本，学会了**遵循指令**的模式。不需要例子的情况下，也能照做并完成新任务。

2. **少样本学习**

给模型几个例子：
  - “苹果 -> 사과”
  - “跑 -> 뛰다”
  - “现在请翻译：吃”

模型内部的归纳头（Induction Head）被激活，识别出这是“一种映射规律”，并把这种规律应用到新的输入上。推理并完成新任务。

上下文工程由此被催生。

- **不仅仅是写一句话**：现在的工程不仅仅是写一个好的提示词，更是构建一个**动态系统**。

- **组装上下文**：在调用模型前，系统可能会加载系统提示词、检索知识库进行增强（RAG）、读取 Skills、提供工具定义等。

- **成本与效率**：在有限的上下文窗口内，塞进最有用的信息变成了系统追求的目标。

# 总结

大模型之所以能做好推理工作，是因为这些任务的本质逻辑已被“压缩”进了它在预训练阶段进行下一个词预测的概率分布中。

提示词工程作为新的“产业”，是因为我们发现通过精心设计的“上下文”可以在不重新训练模型的情况下，激发大模型的隐藏状态，指挥大模型完成推理并解决问题。